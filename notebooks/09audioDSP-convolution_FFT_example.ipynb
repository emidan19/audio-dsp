{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> PROCESAMIENTO DIGITAL DE SEÑALES DE AUDIO</center>\n",
    "## <center> Convolution FFT</center>      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** *Las siguientes dos celdas solo son necesarias para descargar el archivo de ejemplo. Ignórelas si va a trabajar con sus propios archivos de audio.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "#### Descripción\n",
    "\n",
    "En este ejercicio se implementa la convolución rápida usando la FFT. La convolución rápida hace uso de la convolución circular de secuencias de largo finito implementada como el producto de sus transformadas de Fourier. Se procesan tramas sucesivas de la señal de audio de entrada, se calcula el espectro de cada trama y se calcula el producto del espectro de la trama de audio con el espectro del núcleo de convolución (p. ej. la respuesta al impulso de un filtro). Luego se antitransforma el resultado y se combinan tramas sucesivas usando el métodos de Overlap-Add. \n",
    "\n",
    "#### Objetivo\n",
    "\n",
    "El objetivo es lograr la simulación de espacios acústicos, haciendo la convolución de una señal de audio de entrada con la respuesta al impulso de un espacio acústico. De este modo se transfieren las características acústicas del espacio a la señal de audio (por ejemplo su reverberación), como si estuviese sonando en ese lugar.\n",
    "\n",
    "#### Señales de audio y respuestas al impulso\n",
    "\n",
    "Las señales de audio y las respuestas al impulso que se usan en este ejercicio provienen del proyecto OpenAir https://openairlib.net/. Las señales de audio de entrada fueron grabadas en cámara anecoica, de forma que no contengan información de espacialidad. Las respuestas al impulso (IRs) fueron grabadas en diferentes espacios acústicos. En la página del proyecto OpenAir se puede acceder a muchas otras señales de audio y IRs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo correr el notebook\n",
    "Se puede bajar y correr el notebook de forma local en una computadora.\n",
    "\n",
    "O también se puede correr en Google Colab usando el siguiente enlace. \n",
    "\n",
    "<table align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/mrocamora/audio-dsp/blob/main/notebooks/audioDSP-convolution_FFT_example.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1\n",
    "\n",
    "En esta parte se cargan las señales correspondientes a la respuesta al impulso y a la señal de audio a procesar.\n",
    "\n",
    "#### Parte 1.1\n",
    "\n",
    "Luego de cargar la respuesta al impulso responda lo siguiente. \n",
    "\n",
    "1. El archivo de audio de la IR tiene dos canales. ¿Corresponde a dos respuestas al impulso?\n",
    "2. Escuche la señal de audio. ¿Suena como una respuesta al impulso? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download impulse responses as audio files\n",
    "wget.download('https://github.com/mrocamora/audio-dsp/blob/main/audio/IR_data/1st_baptist.wav?raw=true')\n",
    "wget.download('https://github.com/mrocamora/audio-dsp/blob/main/audio/IR_data/york-minster.wav?raw=true')\n",
    "wget.download('https://github.com/mrocamora/audio-dsp/blob/main/audio/IR_data/small-plate-01.wav?raw=true')\n",
    "wget.download('https://github.com/mrocamora/audio-dsp/blob/main/audio/IR_data/inside-a-box.wav?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the audio file of the Impulse Response (IR) (from https://openairlib.net/)\n",
    "filename = '1st_baptist.wav'\n",
    "#filename = 'york-minster.wav'\n",
    "# filename = 'small-plate-01.wav'\n",
    "#filename = 'inside-a-box.wav'\n",
    "\n",
    "# load audio file\n",
    "fs_h, h = wavfile.read(filename)\n",
    "\n",
    "# normalize maximum (absolute) amplitude\n",
    "h = h / np.max(abs(h)) * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time corresponding to the audio signal\n",
    "time_h = np.arange(0, h.shape[0])/fs_h\n",
    "\n",
    "# plot the two channels of the IR audio signal\n",
    "plt.figure(figsize=(12,6))\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(time_h, h[:,0])\n",
    "ax1 = plt.subplot(2, 1, 2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(time_h, h[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio([h[:,0], h[:,1]], rate=fs_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1.2\n",
    "\n",
    "Luego de cargar la señal de audio a procesar responda lo siguiente. \n",
    "\n",
    "1. Escuche la señal de audio. ¿Suena como si hubiese sido grabada en algún espacio acústico en particular? \n",
    "2. ¿Cómo se generan este tipo de señales de audio? ¿Qué ocurriría con señales de audio muy reverberadas?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download audio files\n",
    "wget.download('https://github.com/mrocamora/audio-dsp/blob/main/audio/singing_voice.wav?raw=true')\n",
    "wget.download('https://github.com/mrocamora/audio-dsp/blob/main/audio/trumpet.wav?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the anechoic audio file to process (from https://openairlib.net/)\n",
    "filename = 'singing_voice.wav'\n",
    "# filename = 'trumpet.wav'\n",
    "\n",
    "# read audio file\n",
    "fs, x = wavfile.read(filename)\n",
    "\n",
    "# normalize maximum (absolute) amplitude\n",
    "x = x / np.max(abs(x)) * 0.9\n",
    "\n",
    "# check that sampling rates match\n",
    "assert fs_h == fs, \"Sampling frequencies do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time corresponding to the audio signal\n",
    "time_x = np.arange(0, x.size)/fs\n",
    "\n",
    "# plot the two channels of the IR audio signal\n",
    "plt.figure(figsize=(12,6))\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "plt.plot(time_x, x)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(x, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2\n",
    "\n",
    "Complete el código de la función `convolution_FFT`, que implementa la convolución rápida. Tenga en cuenta que la respuesta al impulso es estéreo, de modo que se obtiene una señal para el canal izquierdo y otra para el canal derecho.\n",
    "\n",
    "Se debe completar lo siguiente:\n",
    "\n",
    "1. Establecer el valor de $N$ (cantidad de puntos usados en la DFT), a partir del largo de una trama de la señal de entrada y la respuesta al impulso.\n",
    "2. Calcular la DFT de una trama de la señal de entrada, el producto con la DFT de las IR y la DFT inversa.\n",
    "3. Acumular adecuadamente la antitransformada de tramas sucesivas, de acuerdo al algoritmo de **overlap-add**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_FFT(x, h, fs=22050, win=0.060):\n",
    "    \"\"\" compute the convolution of a single channel input signal x and a two channel impulse response h\n",
    "        using a fast implementation based on the FFT\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x   : numpy array\n",
    "          input audio file (mono) as a numpy 1D array.\n",
    "    h   : numpy array\n",
    "          input response audio file (stereo) as a numpy 2D array.\n",
    "    fs  : float\n",
    "          sampling rate.\n",
    "    win : float\n",
    "          window length in milliseconds.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy array\n",
    "        output audio file (stereo) as a numpy 2D array.\n",
    "    \"\"\"\n",
    "    \n",
    "    # length of the input signal\n",
    "    M = x.size;\n",
    "    # length of the filter kernel (i.e. IR)\n",
    "    K = h.shape[0]\n",
    "    \n",
    "    # length of the analysis window in samples\n",
    "    L = int(win * fs)\n",
    "    \n",
    "    # number of points to compute the DFT (FFT)\n",
    "    # N = \n",
    "  \n",
    "    # set N to next power of 2 for FFT efficiency\n",
    "    nextPower2 = math.ceil(math.log(N,2))\n",
    "    N = int(math.pow(2, nextPower2))\n",
    "    \n",
    "    # DFT of the IRs\n",
    "    H = np.zeros((N, 2), dtype = complex) # two channels (defined as complex to avoid warnings)\n",
    "    H[:, 0] = np.fft.fft(h[:,0], N) # left channel\n",
    "    H[:, 1] = np.fft.fft(h[:,1], N) # right channel\n",
    "    \n",
    "    # total number of analysis frames\n",
    "    num_frames = int(np.floor(M / L))\n",
    "\n",
    "    # initialize output frame\n",
    "    yr = np.zeros((N, 2))\n",
    "    # initialize DFT of frame\n",
    "    Yr = np.zeros((N, 2), dtype = complex) # two channels (defined as complex to avoid warnings)\n",
    "    # initialize whole output signal\n",
    "    y = np.zeros(((num_frames - 1) * L + N + K - 1, 2))\n",
    "    \n",
    "    # process each frame\n",
    "    for ind in range(num_frames):\n",
    "\n",
    "        # initial and ending points of the frame\n",
    "        n_ini = int(ind * L)\n",
    "        n_end = n_ini + L\n",
    "\n",
    "        # signal frame\n",
    "        xr = x[n_ini:n_end]\n",
    "\n",
    "        # DFT of the signal frame\n",
    "        # Xr = \n",
    "\n",
    "        # product of the DFTs\n",
    "        # Yr[:, 0] =            # left channel\n",
    "        # Yr[:, 1] =            # right channel\n",
    "\n",
    "        # inverse DFT (real part is taken since complex part should be zero)\n",
    "        # yr[:, 0] =            # left channel\n",
    "        # yr[:, 1] =            # right channel\n",
    "        \n",
    "        # overlap-add\n",
    "        # y[?, 0] += yr[:, 0] # left channel\n",
    "        # y[?, 1] += yr[:, 1] # right channel\n",
    "          \n",
    "    # normalize output amplitude to match input amplitude\n",
    "    max_in = np.max(np.abs(x))\n",
    "    max_out = np.max(np.abs(y))\n",
    "    y = y * max_in / max_out                  \n",
    "   \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3\n",
    "\n",
    "Una vez completada la implementación de la función `convolution_FFT` siga los siguientes pasos.\n",
    "\n",
    "1. Ejecute la función `convolution_FFT` para una cierta señal de audio de entrada y una cierta respuesta al impulso. Escuche el resultado de la convolución y grafique la señal de audio obtenida. Compare con la señal de audio original.\n",
    "2. Estime el tiempo que demora el proceso usando la función `time.time()`. \n",
    "3. Repita el proceso pero sin usar un valor de $N$ que sea potencia de 2 (comente el bloque de código que usa `nextPower2` para establecer el valor de $N$). Compare los tiempos de ejecución. ¿Hay alguna diferencia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the convolution using the FFT implementation\n",
    "t = time.time()\n",
    "\n",
    "y_FFT = convolution_FFT(x, h)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio([y_FFT[:, 0], y_FFT[:, 1]], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time corresponding to the output signal\n",
    "time_y_FFT = np.arange(0, y_FFT.shape[0])/fs\n",
    "\n",
    "# plot the two channels of the output signal\n",
    "plt.figure(figsize=(12,6))\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(time_y_FFT, y_FFT[:, 0])\n",
    "ax1 = plt.subplot(2, 1, 2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(time_y_FFT, y_FFT[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 4\n",
    "\n",
    "La función `convolution_TEMP1` implementa la convolución de las señales en el dominio del tiempo. Analice la implementación y responda las siguientes preguntas. \n",
    "\n",
    "1. ¿Por qué se usa la función `flip` de `numpy`?\n",
    "2. ¿Para qué se usa el arreglo `u` (`numpy array`)?\n",
    "3. ¿Es posible implementar una función más eficiente para la convolución en el tiempo? ¿Cómo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_TEMP1(x, h):\n",
    "    \"\"\" compute the convolution of a single channel input signal x and a two channel impulse response h\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x   : numpy array\n",
    "          input audio file (mono) as a numpy 1D array.\n",
    "    h   : numpy array\n",
    "          input response audio file (stereo) as a numpy 2D array.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy array\n",
    "        output audio file (stereo) as a numpy 2D array.\n",
    "    \"\"\"\n",
    "    \n",
    "    # length of the input signal\n",
    "    M = x.size;\n",
    "    # length of the filter kernel (i.e. IR)\n",
    "    K = h.shape[0]\n",
    "    \n",
    "    # total length of the output signal\n",
    "    T = M + K - 1\n",
    "    \n",
    "    # initialize whole output signal\n",
    "    y = np.zeros((T, 2))\n",
    "\n",
    "    # initialize auxiliary array\n",
    "    u = np.zeros(T + K - 1)\n",
    "    u[K-1:K-1+M] = x\n",
    "\n",
    "    # flip kernel (left and right channels)\n",
    "    h_l = np.flip(h[:, 0])\n",
    "    h_r = np.flip(h[:, 1])    \n",
    "   \n",
    "    # compute the convolution\n",
    "    for t in range(T):\n",
    "\n",
    "        # initialize auxiliary array for filter kernel (left channel)\n",
    "        v = np.zeros(T + K - 1)\n",
    "        v[t:t+K] = h_l\n",
    "        # compute output (left channel)\n",
    "        y[t, 0] = u @ v\n",
    "\n",
    "        # initialize auxiliary array for filter kernel (right channel)\n",
    "        v = np.zeros(T + K - 1)\n",
    "        v[t:t+K] = h_r\n",
    "        # compute output (right channel)\n",
    "        y[t, 1] = u @ v\n",
    "                \n",
    "    # normalize output amplitude to match input amplitude\n",
    "    max_in = np.max(np.abs(x))\n",
    "    max_out = np.max(np.abs(y))\n",
    "    y = y * max_in / max_out\n",
    "                     \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 5\n",
    "\n",
    "Una vez analizada la implementación de la función `convolution_TEMP1` se sugiere realizar lo siguiente.\n",
    "\n",
    "1. Ejecute la función `convolution_TEMP1` para la misma señal de audio de entrada y la misma respuesta al impulso usadas antes. Escuche el resultado de la convolución y grafique la señal de audio obtenida. Compare con la señal de audio original y con la obtenida con la convolución rápida.\n",
    "2. Estime el tiempo que demora el proceso usando la función `time.time()`. \n",
    "3. Compare el tiempo de ejecución con el obtenido usando la convolución rápida (`convolution_FFT`). ¿Hay alguna diferencia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the convolution using the direct implementation in time (version 1)\n",
    "t = time.time()\n",
    "\n",
    "y_TEMP1 = convolution_TEMP1(x, h)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio([y_TEMP1[:, 0], y_TEMP1[:, 1]], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time corresponding to the output signal\n",
    "time_y_TEMP1 = np.arange(0, y_TEMP1.shape[0])/fs\n",
    "\n",
    "# plot the two channels of the output signal\n",
    "plt.figure(figsize=(12,6))\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(time_y_TEMP1, y_TEMP1[:, 0])\n",
    "ax1 = plt.subplot(2, 1, 2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(time_y_TEMP1, y_TEMP1[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 6\n",
    "\n",
    "La función `convolution_TEMP2` también implementa la convolución de las señales en el dominio del tiempo. Analice la implementación y responda las siguientes preguntas. \n",
    "\n",
    "1. ¿Qué representan los índices `x_ini` y `x_end`?\n",
    "2. ¿Qué representan los índices `h_ini` y `h_end`?\n",
    "3. ¿Cree que esta implementación es más o menos eficiente que la anterior (`convolution_TEMP1`)? ¿Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_TEMP2(x, h):\n",
    "    \"\"\" compute the convolution of a single channel input signal x and a two channel impulse response h\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x   : numpy array\n",
    "          input audio file (mono) as a numpy 1D array.\n",
    "    h   : numpy array\n",
    "          input response audio file (stereo) as a numpy 2D array.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy array\n",
    "        output audio file (stereo) as a numpy 2D array.\n",
    "    \"\"\"\n",
    "    \n",
    "    # length of the input signal\n",
    "    M = x.size;\n",
    "    # length of the filter kernel (i.e. IR)\n",
    "    K = h.shape[0]\n",
    "    \n",
    "    # total length of the output signal\n",
    "    T = M + K - 1\n",
    "    \n",
    "    # initialize whole output signal\n",
    "    y = np.zeros((T, 2))\n",
    "    \n",
    "    # flip kernel (left and right channels)\n",
    "    h_l = np.flip(h[:, 0])\n",
    "    h_r = np.flip(h[:, 1])    \n",
    "  \n",
    "    # compute the convolution\n",
    "    for t in range(T):\n",
    "\n",
    "        # initial and ending index for input signal\n",
    "        x_ini = max(0, t + 1 - K)\n",
    "        x_end = min(t + 1, M)\n",
    "\n",
    "        # initial and ending index for filter kernel\n",
    "        h_ini = max(0, K - t - 1)\n",
    "        h_end = min(K, M + K - t - 1)\n",
    "        \n",
    "        # compute output (left and right channel)\n",
    "        y[t, 0] = x[x_ini:x_end] @ h_l[h_ini:h_end]\n",
    "        y[t, 1] = x[x_ini:x_end] @ h_r[h_ini:h_end]\n",
    "              \n",
    "    # normalize output amplitude to match input amplitude\n",
    "    max_in = np.max(np.abs(x))\n",
    "    max_out = np.max(np.abs(y))\n",
    "    y = y * max_in / max_out\n",
    "                     \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 7\n",
    "\n",
    "Una vez analizada la implementación de la función `convolution_TEMP2` se sugiere realizar lo siguiente.\n",
    "\n",
    "1. Ejecute la función `convolution_TEMP2` para la misma señal de audio de entrada y la misma respuesta al impulso usadas antes. Escuche el resultado de la convolución y grafique la señal de audio obtenida. Compare con la señal de audio original y con la obtenida con la convolución rápida.\n",
    "2. Estime el tiempo que demora el proceso usando la función `time.time()`. \n",
    "3. Compare el tiempo de ejecución con el obtenido usando la función `convolution_TEMP1`. ¿Hay alguna diferencia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the convolution using the direct implementation in time (version 2)\n",
    "t = time.time()\n",
    "\n",
    "y_TEMP2 = convolution_TEMP2(x, h)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio([y_TEMP2[:, 0], y_TEMP2[:, 1]], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time corresponding to the output signal\n",
    "time_y_TEMP2 = np.arange(0, y_TEMP2.shape[0])/fs\n",
    "\n",
    "# plot the two channels of the output signal\n",
    "plt.figure(figsize=(12,6))\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(time_y_TEMP2, y_TEMP2[:, 0])\n",
    "ax1 = plt.subplot(2, 1, 2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(time_y_TEMP2, y_TEMP2[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 8\n",
    "\n",
    "Por último tiene sentido comparar en detalle las señales obtenidas. Para estoo se sugiere sumar el valor absoluto de la diferencia de las tres señales calculadas (`y_FFT`, `y_TEMP1` e `y_TEMP2`). \n",
    "\n",
    "**Nota:** tenga en cuenta que el largo de las señales puede no ser el mismo. \n",
    "\n",
    "Una vez hecha la comparación responda las siguientes preguntas.\n",
    "\n",
    "1. ¿Cuál es la diferencia entre las señales obtenidas usando `convolution_TEMP1` y `convolution_TEMP2`? \n",
    "2. ¿De qué orden es el ruido de las operaciones? ¿Cuántas muestras tiene la señal?\n",
    "2. ¿Cuál es la diferencia entre las señales obtenidas usando `convolution_FFT` y `convolution_TEMP1`? \n",
    "3. ¿Qué implementación realiza menos operaciones `convolution_FFT` o `convolution_TEMP1`? \n",
    "4. ¿Qué implementación cree que acumula menos ruido de operaciones? ¿Qué implementación es más precisa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_FFT.shape, y_TEMP1.shape, y_TEMP2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
